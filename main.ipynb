{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530b69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.7\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -q langchain langchain-openai langchain-core langchain-community docx2txt pypdf langchain-chroma sentence_transformers python-dotenv\n",
    "\n",
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded03273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.smith.langchain.com\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.environ[\"LANGSMITH_ENDPOINT\"])\n",
    "print(os.environ[\"LANGCHAIN_TRACING_V2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e03712",
   "metadata": {},
   "source": [
    "### What is Retrieval Augmented Generation (RAG)?\n",
    "\n",
    "RAG is a technique that enhances language models by combining them with a retrieval system. It allows the model to access and utilize external knowledge when generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e305085",
   "metadata": {},
   "source": [
    "### Overview of Langchain\n",
    "\n",
    "Langchain is a framework for developing applications powered by language models. It provides a set of tools and abstractions that make it easier to build complex AI applications. Key features include:\n",
    "\n",
    "- Modular components for common LLM tasks\n",
    "- Built-in support for various LLM providers\n",
    "- Tools for document loading, text splitting, and vector storage\n",
    "- Abstractions for building conversational agents and question-answering systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f59bb",
   "metadata": {},
   "source": [
    "Call LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd6e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st llm_response: content='Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 14, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D4tuCTUnmaStWItXhGfci2SJ6FUwD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c1fd5-128e-7de1-861e-a4b552e37283-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "2nd llm_response: Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm_response = llm.invoke(\"Tell me a joke about programming.\")\n",
    "print(f\"1st llm_response: {llm_response}\")\n",
    "print(f\"2nd llm_response: {llm_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c36460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccb9424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do programmers prefer dark mode? Because the light attracts bugs!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = llm | output_parser\n",
    "chain.invoke(\"Tell me a joke about programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:2073: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileReview(phone_model='Galaxy S21', rating=4, pros=['Gorgeous screen with vibrant colors', 'Impressive camera performance, especially at night', 'Solid battery life'], cons=['Pricey', 'No charger included', 'New button layout takes getting used to'], summary='The Galaxy S21 is a great phone with a few quirks. It has a gorgeous screen, impressive camera, and solid battery life. However, the high price, lack of included charger, and new button layout may be drawbacks for some users. Overall, a solid 4 out of 5 rating - worth checking out for those due for an upgrade.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MobileReview(BaseModel):\n",
    "  phone_model: str = Field(description=\"The name and model of the mobile phone being reviewed\")\n",
    "  rating: int = Field(description=\"The rating of the mobile review from 1 to 5\")\n",
    "  pros: List[str] = Field(description=\"A list of positive things mentioned in the mobile review\")\n",
    "  cons: List[str] = Field(description=\"A list of negative things mentioned in the mobile review\")\n",
    "  summary: str = Field(description=\"Brief summary of the mobile review\")\n",
    "\n",
    "review_text = \"\"\"\n",
    "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous, colors pop like crazy.\n",
    "Camera's insane too, especially at night - my Insta game's never been stronger. Battery life is solid, easily lasts me all day with heavy use.\n",
    "\n",
    "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
    "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
    "\n",
    "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from being perfect.\n",
    "If you're due for an upgrade, definitely worth checkng out.\n",
    "\"\"\"\n",
    "\n",
    "structured_llm = llm.with_structured_output(MobileReview)\n",
    "output = structured_llm.invoke(review_text)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1584ef",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da1e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me a joke about programming.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template([\n",
    "#     (\"system\", \"You are a helpful assistant that extracts structured data from mobile phone reviews.\"),\n",
    "#     (\"user\", \"Extract the following review into structured data:\\n{review_text}\")\n",
    "# ])\n",
    "prompt.invoke({\"topic\": \"programming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8cfbaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the car driver bring a map to the bar? \\nBecause he heard the drinks were on the house!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"topic\": \"car drivers\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2af854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the robot go on a diet? \n",
      "\n",
      "Because it had too many bytes!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# define the prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# Initilize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Compose the chain\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Use the chain\n",
    "result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9754cd6",
   "metadata": {},
   "source": [
    "## LLM Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3272cdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why was the robot so bad at soccer?\\n\\nBecause it kept kicking up sparks instead of the ball!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 27, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1590f93f9d', 'id': 'chatcmpl-D4z7G7Mvr4bJGeWJUKvQJGzg0zDxj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c2106-b33b-7420-9f12-78439452bc24-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 27, 'output_tokens': 20, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant that tells jokes.\")\n",
    "human_message = HumanMessage(content=\"Tell me a joke about robots.\")\n",
    "llm.invoke([system_message, human_message])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
