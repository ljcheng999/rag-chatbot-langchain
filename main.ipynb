{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530b69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.7\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -q langchain langchain-openai langchain-core langchain-community docx2txt pypdf langchain-chroma sentence_transformers python-dotenv\n",
    "\n",
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded03273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.smith.langchain.com\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.environ[\"LANGSMITH_ENDPOINT\"])\n",
    "print(os.environ[\"LANGCHAIN_TRACING_V2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e03712",
   "metadata": {},
   "source": [
    "### What is Retrieval Augmented Generation (RAG)?\n",
    "\n",
    "RAG is a technique that enhances language models by combining them with a retrieval system. It allows the model to access and utilize external knowledge when generating responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e305085",
   "metadata": {},
   "source": [
    "### Overview of Langchain\n",
    "\n",
    "Langchain is a framework for developing applications powered by language models. It provides a set of tools and abstractions that make it easier to build complex AI applications. Key features include:\n",
    "\n",
    "- Modular components for common LLM tasks\n",
    "- Built-in support for various LLM providers\n",
    "- Tools for document loading, text splitting, and vector storage\n",
    "- Abstractions for building conversational agents and question-answering systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f59bb",
   "metadata": {},
   "source": [
    "Call LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd6e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st llm_response: content='Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 14, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-D4tuCTUnmaStWItXhGfci2SJ6FUwD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c1fd5-128e-7de1-861e-a4b552e37283-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 14, 'output_tokens': 12, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "2nd llm_response: Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm_response = llm.invoke(\"Tell me a joke about programming.\")\n",
    "print(f\"1st llm_response: {llm_response}\")\n",
    "print(f\"2nd llm_response: {llm_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c36460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccb9424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do programmers prefer dark mode? Because the light attracts bugs!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = llm | output_parser\n",
    "chain.invoke(\"Tell me a joke about programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94e44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:2073: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileReview(phone_model='Galaxy S21', rating=4, pros=['Gorgeous screen with vibrant colors', 'Impressive camera performance, especially at night', 'Solid battery life'], cons=['Pricey', 'No charger included', 'New button layout takes getting used to'], summary='The Galaxy S21 is a great phone with a few quirks. It has a gorgeous screen, impressive camera, and solid battery life. However, the high price, lack of included charger, and new button layout may be drawbacks for some users. Overall, a solid 4 out of 5 rating - worth checking out for those due for an upgrade.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MobileReview(BaseModel):\n",
    "  phone_model: str = Field(description=\"The name and model of the mobile phone being reviewed\")\n",
    "  rating: int = Field(description=\"The rating of the mobile review from 1 to 5\")\n",
    "  pros: List[str] = Field(description=\"A list of positive things mentioned in the mobile review\")\n",
    "  cons: List[str] = Field(description=\"A list of negative things mentioned in the mobile review\")\n",
    "  summary: str = Field(description=\"Brief summary of the mobile review\")\n",
    "\n",
    "review_text = \"\"\"\n",
    "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous, colors pop like crazy.\n",
    "Camera's insane too, especially at night - my Insta game's never been stronger. Battery life is solid, easily lasts me all day with heavy use.\n",
    "\n",
    "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
    "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
    "\n",
    "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from being perfect.\n",
    "If you're due for an upgrade, definitely worth checkng out.\n",
    "\"\"\"\n",
    "\n",
    "structured_llm = llm.with_structured_output(MobileReview)\n",
    "output = structured_llm.invoke(review_text)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1584ef",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da1e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me a joke about programming.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template([\n",
    "#     (\"system\", \"You are a helpful assistant that extracts structured data from mobile phone reviews.\"),\n",
    "#     (\"user\", \"Extract the following review into structured data:\\n{review_text}\")\n",
    "# ])\n",
    "prompt.invoke({\"topic\": \"programming\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8cfbaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the car driver bring a map to the bar? \\nBecause he heard the drinks were on the house!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"topic\": \"car drivers\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2af854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the robot go on a diet? \n",
      "\n",
      "Because it had too many bytes!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# define the prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# Initilize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Compose the chain\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Use the chain\n",
    "result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9754cd6",
   "metadata": {},
   "source": [
    "## LLM Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3272cdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why was the robot so bad at soccer?\\n\\nBecause it kept kicking up sparks instead of the ball!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 27, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1590f93f9d', 'id': 'chatcmpl-D4z7G7Mvr4bJGeWJUKvQJGzg0zDxj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c2106-b33b-7420-9f12-78439452bc24-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 27, 'output_tokens': 20, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant that tells jokes.\")\n",
    "human_message = HumanMessage(content=\"Tell me a joke about robots.\")\n",
    "llm.invoke([system_message, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9445591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that tells jokes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about quantum computing.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate([\n",
    "  (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "  (\"user\", \"Tell me a joke about {topic}.\")\n",
    "])\n",
    "prompt_values = template.invoke({\n",
    "  \"topic\": \"quantum computing\"\n",
    "})\n",
    "prompt_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe7bc8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the quantum computer break up with its partner?\\n\\nBecause it couldn’t find a stable state!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 28, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1590f93f9d', 'id': 'chatcmpl-D4zZkpLhzj5gFB2UY3RGxI7Qy0gL2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c2121-a5e5-7c11-a037-f01c58369cbe-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 28, 'output_tokens': 20, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd2b7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install docx2txt pypdf unstructured sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac0bc4",
   "metadata": {},
   "source": [
    "## Document Processing for RAG Systems\n",
    "After setting up our LangChain components, the next crucial step in building a RAG system is processing our documents. This involves loading the documents and splitting them into manageable chunks.\n",
    "\n",
    "### Loading Documents\n",
    "We start by loading documents from various file types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f34e217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 documents from the folder.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "  documents = []\n",
    "  for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if filename.endswith('.pdf'):\n",
    "      loader = PyPDFLoader(file_path)\n",
    "    elif filename.endswith('.docx'):\n",
    "      loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "      print(f\"Unsupported file type: {filename}\")\n",
    "      continue\n",
    "    documents.extend(loader.load())\n",
    "  return documents\n",
    "\n",
    "folder_path = \"docs\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4146a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-11-06T10:08:55+00:00', 'author': '', 'keywords': '', 'moddate': '2024-11-06T10:08:55+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/langchain_turing.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe,andLangSmith.Weexplorehowtheframeworkfacilitatesthe\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models· LLM Applications ·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e249851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.26',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2024-11-06T10:08:55+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2024-11-06T10:08:55+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'docs/langchain_turing.pdf',\n",
       " 'total_pages': 14,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb7d70b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain\\nVasilios Mavroudis\\nAlan Turing Institute\\nvmavroudis@turing.ac.uk\\nAbstract. LangChain is a rapidly emerging framework that offers a ver-\\nsatile and modular approach to developing applications powered by large\\nlanguage models (LLMs). By leveraging LangChain, developers can sim-\\nplify complex stages of the application lifecycle—such as development,\\nproductionization, and deployment—making it easier to build scalable,\\nstateful, and contextually aware applications. It provides tools for han-\\ndling chat models, integrating retrieval-augmented generation (RAG),\\nand offering secure API interactions. With LangChain, rapid deployment\\nof sophisticated LLM solutions across diverse domains becomes feasible.\\nHowever, despite its strengths, LangChain’s emphasis on modularity and\\nintegration introduces complexities and potential security concerns that\\nwarrant critical examination. This paper provides an in-depth analysis\\nof LangChain’s architecture and core components, including LangGraph,\\nLangServe,andLangSmith.Weexplorehowtheframeworkfacilitatesthe\\ndevelopment of LLM applications, discuss its applications across multi-\\nple domains, and critically evaluate its limitations in terms of usability,\\nsecurity, and scalability. By offering valuable insights into both the capa-\\nbilities and challenges of LangChain, this paper serves as a key resource\\nfor developers and researchers interested in leveraging LangChain for\\ninnovative and secure LLM-powered applications.\\nKeywords: LangChain · Large Language Models· LLM Applications ·\\nModular Framework\\nThe emergence of large language models (LLMs) such as OpenAI’s o1 [13],\\nGPT-4o [12], Google’s Gemini [14], and Meta’s LLaMA [16] has revolutionized\\nthe field of natural language processing (NLP). These advanced models have un-\\nlocked unprecedented capabilities in understanding and generating human-like\\ntext, enabling applications that range from intelligent conversational agents to\\nsophisticated data analysis tools. However, harnessing the full potential of LLMs\\nin real-world applications presents significant challenges. Developers must nav-\\nigate complexities related to model integration, state management, scalability,\\ncontextual awareness, and security.\\nLangChain has rapidly gained prominence as a powerful framework designed\\nto address these challenges in developing LLM-powered applications [2]. By\\nproviding a modular and flexible architecture, LangChain simplifies the com-\\nplexities inherent in working with LLMs, enabling developers to build scalable,'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4b50c",
   "metadata": {},
   "source": [
    "### Splitting Documents\n",
    "Next, we split these documents into smaller, more manageable chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "026004fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document(s) into 49 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the document(s) into {len(splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584582b",
   "metadata": {},
   "source": [
    "## Creating Embeddings for RAG Systems\n",
    "After processing our documents, the next crucial step is to create embeddings. Embeddings are vector representations of our text chunks that allow for efficient similarity search, which is key to the retrieval part of our RAG system.\n",
    "\n",
    "#### Using OpenAI Embeddings\n",
    "First, let's look at how we can create embeddings using OpenAI's embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f30598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 49 document chunks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0014729297254234552,\n",
       " 0.015365492552518845,\n",
       " -0.010614775121212006,\n",
       " -0.03759082779288292,\n",
       " 0.005141423549503088]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
    "print(f\"Created embeddings for {len(document_embeddings)} document chunks.\")\n",
    "document_embeddings[0][:5] ### only print first 5 values of the first embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4ff6a",
   "metadata": {},
   "source": [
    "## Setting Up the Vector Store for RAG Systems\n",
    "Now that we have our document embeddings, we need a way to store and efficiently search through them. This is where a vector store comes in. I'll use Chroma, a popular vector store that integrates well with LangChain.\n",
    "\n",
    "## Creating the Vector Store\n",
    "Set up our Chroma vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16f21054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted to './chroma_db'\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "embeddings_function = OpenAIEmbeddings()\n",
    "collection_name = \"my_rag_collection\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embeddings_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348ec1d",
   "metadata": {},
   "source": [
    "## Performing Similarity Search\n",
    "Now that our vector store is set up, we can perform similarity searches. This is a key component of the retrieval process in our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e0bb484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 most relevant chunks for the query: 'When was Langchain founded?'\n",
      "\n",
      "Result 1:\n",
      "Source: docs/langchain_turing.pdf\n",
      "Content: LangChain\n",
      "Vasilios Mavroudis\n",
      "Alan Turing Institute\n",
      "vmavroudis@turing.ac.uk\n",
      "Abstract. LangChain is a rapidly emerging framework that offers a ver-\n",
      "satile and modular approach to developing applications powered by large\n",
      "language models (LLMs). By leveraging LangChain, developers can sim-\n",
      "plify complex stages of the application lifecycle—such as development,\n",
      "productionization, and deployment—making it easier to build scalable,\n",
      "stateful, and contextually aware applications. It provides tools for han-\n",
      "dling chat models, integrating retrieval-augmented generation (RAG),\n",
      "and offering secure API interactions. With LangChain, rapid deployment\n",
      "of sophisticated LLM solutions across diverse domains becomes feasible.\n",
      "However, despite its strengths, LangChain’s emphasis on modularity and\n",
      "integration introduces complexities and potential security concerns that\n",
      "warrant critical examination. This paper provides an in-depth analysis\n",
      "of LangChain’s architecture and core components, including LangGraph,\n",
      "\n",
      "Result 2:\n",
      "Source: docs/langchain_turing.pdf\n",
      "Content: 12 Vasilios Mavroudis\n",
      "of the API. LangChain workflows, tools, and chains can be directly exposed\n",
      "via LangServe endpoints, providing flexible API interactions and enhancing the\n",
      "functionality of LLM applications [5].\n",
      "5 Limitations and Criticisms\n",
      "LangChain provides a versatile framework for the development of applications\n",
      "powered by large language models (LLMs). However, several limitations warrant\n",
      "attention, especially in the domains of complexity and security.\n",
      "5.1 Complexity\n",
      "LangChain’s modular architecture, while designed to simplify LLM-based ap-\n",
      "plication development, can paradoxically increase complexity. Effective use of\n",
      "LangChain often requires a nuanced understanding of its distinct components,\n",
      "such as LangGraph and LangSmith, as well as familiarity with its API ecosys-\n",
      "tem. Consequently, developers may face a steep learning curve, particularly those\n",
      "aiming for rapid prototyping or deployment. The need for comprehensive knowl-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"When was Langchain founded?\"\n",
    "search_results = vectorstore.similarity_search(query, k=2) # retrieve top 2 most relevant documents\n",
    "print(f\"\\nTop 2 most relevant documents for the query: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b874f70",
   "metadata": {},
   "source": [
    "### Creating a Retriever\n",
    "We can also create a retriever from our vector store, which will be useful when we build our full RAG chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "790d7508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='9cbaf462-8fd6-4899-876f-1941c087620d', metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': '', 'title': '', 'producer': 'pdfTeX-1.40.26', 'moddate': '2024-11-06T10:08:55+00:00', 'page_label': '14', 'subject': '', 'keywords': '', 'source': 'docs/langchain_turing.pdf', 'creator': 'LaTeX with hyperref', 'trapped': '/False', 'page': 13, 'creationdate': '2024-11-06T10:08:55+00:00', 'total_pages': 14}, page_content='11. Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert, Ilan\\nHorn, Naty Leiser, and Grzegorz Czajkowski. Pregel: A System for Large-Scale\\nGraph Processing. InProceedings of the 2010 ACM SIGMOD International Con-\\nference on Management of Data, pages 135–146, 2010.\\n12. OpenAI. Hello GPT-4O, 05 2024.\\n13. OpenAI. Introducing OpenAI O1-Preview, 09 2024.\\n14. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui\\nYu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican,\\net al. Gemini: A Family of Highly Capable Multimodal Models.arXiv preprint\\narXiv:2312.11805, 2023.\\n15. The Apache Software Foundation.Apache Beam: An Advanced Unified Program-\\nming Model, 2024. Accessed: 2024-11-04.\\n16. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, et al. LLaMA: Open and Efficient Foundation Language Models.arXiv'), Document(id='06d2f8e9-5f09-4555-936b-03c2f91b1871', metadata={'creator': 'LaTeX with hyperref', 'page_label': '13', 'producer': 'pdfTeX-1.40.26', 'page': 12, 'author': '', 'subject': '', 'title': '', 'total_pages': 14, 'keywords': '', 'trapped': '/False', 'moddate': '2024-11-06T10:08:55+00:00', 'source': 'docs/langchain_turing.pdf', 'creationdate': '2024-11-06T10:08:55+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='ployment, and LangSmith for monitoring and evaluation—enables developers to\\nbuild scalable, context-aware applications tailored to specific needs across di-\\nverse domains, including NLP, cybersecurity, healthcare, finance, and customer\\nservice.\\nWhile its versatility extends beyond NLP, allowing for applications in fields\\nlike cybersecurity (e.g., threat detection and automated incident response), the\\nframework’s emphasis on flexibility introduces complexities that may present a\\nlearning curve for developers new to LangChain. Additionally, reliance on exter-\\nnal integrations raises important security considerations, such as data exposure\\nand dependency vulnerabilities, which are critical in sensitive areas where data\\nintegrity and privacy are paramount.\\nIn summary, LangChain’s transformative potential lies in bridging the gap\\nbetween the power of large language models and practical application develop-\\nment across multiple fields. By balancing its robust capabilities with enhance-')]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retriever_results = retriever.invoke(\"When was GreenGrow Innovations founded?\")\n",
    "print(retriever_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49a4e49",
   "metadata": {},
   "source": [
    "The retriever provides a convenient interface for retrieving relevant documents, which we'll use when constructing our RAG pipeline.\n",
    "\n",
    "By setting up this vector store, we've created a powerful tool for quickly finding relevant information from our document collection. This is a crucial component of our RAG system, enabling it to retrieve context-relevant information to support the language model's responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5685b65",
   "metadata": {},
   "source": [
    "## Creating the RAG Chain\n",
    "Let's construct our RAG chain using LangChain components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60cff092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not mention when LangChain was founded.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"When was Langchain founded?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
